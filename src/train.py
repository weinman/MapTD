# MapTD
# Copyright (C) 2018 Nathan Gifford, Abyaya Lamsal, Jerod Weinman
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# train.py -- Train the model from scratch or an existing checkpoint.

import numpy as np
import tensorflow as tf
from tensorflow.contrib import slim

import model
import pipeline

tf.app.flags.DEFINE_string('output', '../data/model',
                           """Directory for event logs and checkpoints""")
tf.app.flags.DEFINE_string('tune_from','',
                           """Path to pre-trained model checkpoint""")

tf.app.flags.DEFINE_integer('tile_size', 2**9,
                            """Length of input image square tile edge""")
tf.app.flags.DEFINE_integer('num_gpus', 1,
                            """Number of GPUs to use for distributed training""")

tf.app.flags.DEFINE_integer('batch_size', 2**4,
                            """Mini-batch size""")
tf.app.flags.DEFINE_float('learning_rate',1e-4,
                          """Initial learning rate""")
tf.app.flags.DEFINE_float('min_learning_rate',None,
                          """Initial learning rate""")
tf.app.flags.DEFINE_float('momentum',0.9,
                          """Optimizer gradient first-order momentum""")
tf.app.flags.DEFINE_float('decay_rate',0.9,
                          """Learning rate decay base""")
tf.app.flags.DEFINE_float('decay_steps',2**16,
                          """Learning rate decay exponent scale""")
tf.app.flags.DEFINE_boolean('decay_staircase',False,
                          """Staircase learning rate decay by integer division""")
tf.app.flags.DEFINE_integer('max_num_steps', 2**21,
                            """Number of optimization steps to run""")
tf.app.flags.DEFINE_integer('save_checkpoint_secs', 480,
                            """Interval between saving checkpoints""")

tf.app.flags.DEFINE_string('image_path','../data/images',
                           """Base directory for image training data""")
tf.app.flags.DEFINE_string('gt_path','../data/json',
                           """Base directory for ground truth training data""")
tf.app.flags.DEFINE_string('filename_pattern','*',
                           """File pattern for training input data""")

tf.app.flags.DEFINE_string('validation_filename_pattern',None,
                           """File pattern for validation data""")
tf.app.flags.DEFINE_integer('validation_steps',100,
                            """Number of steps for validation evaluations""")
tf.app.flags.DEFINE_integer('validation_throttle_secs',420,
                            """Minimum seconds between validation evaluations""")

FLAGS = tf.app.flags.FLAGS

tf.logging.set_verbosity( tf.logging.INFO )

def _get_loss(images, score_maps, geo_maps, training_masks, 
               reuse_variables=None):
    """
    Get the scalar tensor that calculates current loss of model 

    Parameters
      images         : TensorFlow placeholder for image batch (the input image
                         tiles the model is training on)
      score_maps     : tensor of ground truth score maps with shape 
                         [batch_size, tile_size/4, tile_size/4, 1]
                         (cf. targets.generate)
      geo_maps       : tensor of ground truth geometry maps with shape 
                         [batch_size, tile_size/4, tile_size/4, 5]
                         (cf. targets.generate)
      training_masks : binary tensor to indicate which locations should be 
                         included in the loss calculations
      reuse_variables: boolean for allowing variable reuse within current scope
                       true to enable reuse, disabled otherwise (default)
    Returns
      loss, a scalar tensor
    """
    # Build inference graph
    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):
        f_score, f_geometry = model.outputs(images, is_training=True)

    # calculate loss
    model_loss = model.loss(score_maps, f_score, geo_maps, f_geometry, 
                            training_masks)

    # Account for the network's loss generated by the regularization function
    #total_loss = tf.add_n([model_loss] + 
    #                      tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))

    # Make snapshot of current ground truth and predictions of model
    if False:
        tf.summary.image('input', images)
        tf.summary.image('score_map', score_maps)
        tf.summary.image('score_map_pred', f_score * 255)
        tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1])
        tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1])
        tf.summary.image('training_masks', training_masks)

    return model_loss


def _get_train_op(loss):
    """
    Configure training op for tf.estimator.EstimatorSpec, using command-line 
      flags and loss op to setup the optimizer 

    Parameters
      loss     : scalar loss tensor (cf. _get_loss)

    Returns
      train_op : TensorFlow op to optimize loss 
    """
    
    learning_rate = tf.train.exponential_decay(
        learning_rate=FLAGS.learning_rate,
        global_step=tf.train.get_global_step(),
        decay_steps=FLAGS.decay_steps,
        decay_rate=FLAGS.decay_rate,
        staircase=FLAGS.decay_staircase,
        name='decaying_learning_rate' )

    if FLAGS.min_learning_rate:
        learning_rate = tf.maximum( learning_rate, FLAGS.min_learning_rate,
                                    name='learning_rate')
    
    optimizer = tf.train.AdamOptimizer(learning_rate,beta1=FLAGS.momentum)

    
    nn_vars = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES)
    
    train_op = tf.contrib.layers.optimize_loss(
        loss=loss,
        global_step=tf.train.get_global_step(),
        learning_rate=learning_rate, 
        optimizer=optimizer,
        summaries=['learning_rate','loss'], # ,'gradients','gradient_norm'
        variables=nn_vars)

    return train_op


def input_fn(filename_pattern):
    """
    Get batched dataset from the input data pipeline. See pipeline.get_dataset.
    """

    gpu_batch_size = FLAGS.batch_size / FLAGS.num_gpus

    return lambda: pipeline.get_dataset(FLAGS.image_path, #using default image_ext
                                        FLAGS.gt_path,    #using default gt_ext
                                        str.split(filename_pattern,','),
                                        tile_size=FLAGS.tile_size,
                                        batch_size=gpu_batch_size)


def _get_distribution_strategy():
    """
    Configure training distribution strategy (e.g., using one or more GPUs)
    for tf.estimator.RunConfig.
    """
 
    if FLAGS.num_gpus > 1:
        return tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.num_gpus)
    else:
        return None

    
def _get_init_pretrained( tune_from ):
    """
    Return lambda for reading pretrained initial model with a given session
    
    Parameters
      tune_from : path to pre-trained model checkpoint

    Returns
      init_fn   : a function for tf.train.Scaffold that takes a tf.train.Scaffold
                    and tf.Session and side-effects by restoring the model 
                    parameters from the given checkpiont
    """
    
    if not tune_from:
        return None
    
    # Extract the global variables
    saver_reader = tf.train.Saver(
        tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES ) )
    
    ckpt_path=tune_from

    # Function to build the scaffold to initialize the training process
    init_fn = lambda scaffold, sess: saver_reader.restore( sess, ckpt_path )

    return init_fn


def model_fn(features, labels, mode):
    """
    Returns the ops necessary for model training, evaluation, and prediction 
      as needed by tf.estimator.Estimator

    Parameters
      features : a dictionary containing keys
                   'tile'      : tensor of image tiles
                   'geo_map'   : tensor of ground truth geometry maps ,
                   'score_map' : tensor of ground truth score maps 
                   'train_mask': tensor of binary training masks
      labels   : not used, but required by tf.estimator.Estimator
      mode     : a tf.estimator.ModeKeys member value specifying mode as 
                   training, evaluation, or prediction
    """
    tile = features['tile']
    geo = features['geo_map']
    score = features['score_map']
    train_mask = features['train_mask']

    loss = _get_loss(tile, score, geo, train_mask)
    
    train_op = _get_train_op(loss)

    # Initialize weights from a pre-trained model
    # NOTE: Does not work when num_gpus>1, cf. tensorflow issue 21615.
    scaffold = tf.train.Scaffold( init_fn=
                                  _get_init_pretrained( FLAGS.tune_from ) )
    
    return tf.estimator.EstimatorSpec(mode=mode,
                                      loss=loss,
                                      train_op=train_op,
                                      scaffold=scaffold )


def _get_config():
    """Return the tf.estimator.RunConfig with configurations for an Estimator"""
    config = tf.ConfigProto(allow_soft_placement=True)
    custom_config = tf.estimator.RunConfig(
        session_config=config,
        save_checkpoints_secs=FLAGS.save_checkpoint_secs,
        train_distribute=_get_distribution_strategy())

    return custom_config


def main(argv=None):
    """
    Train the model on the images found in image_path and ground truth
    values found at gt_path
    """
    classifier = tf.estimator.Estimator( config=_get_config(),
                                         model_fn=model_fn,
                                         model_dir=FLAGS.output)

    # Without validation, just use standard estimator training
    if not FLAGS.validation_filename_pattern:
        classifier.train( input_fn=input_fn(FLAGS.filename_pattern),
                          max_steps=FLAGS.max_num_steps )
    else:
        # With validation, use train_and_evaluate
        train_spec = tf.estimator.TrainSpec(
            input_fn=input_fn(FLAGS.filename_pattern),
            max_steps=FLAGS.max_num_steps )
        eval_spec = tf.estimator.EvalSpec(
            input_fn=input_fn(FLAGS.validation_filename_pattern),
            steps=FLAGS.validation_steps,
            throttle_secs=FLAGS.validation_throttle_secs)
        
        tf.estimator.train_and_evaluate( classifier, train_spec, eval_spec )
   
if __name__ == '__main__':
    tf.app.run()
